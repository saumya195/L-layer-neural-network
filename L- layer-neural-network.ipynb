{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building deep neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Sign Language Digits Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sign languages (also known as signed languages) are languages that use manual communication to convey meaning. This can include simultaneously employing hand gestures, movement, orientation of the fingers, arms or body, and facial expressions to convey a speaker's ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "Letâ€™s first import all the packages that you will need during this assignment.\n",
    "\n",
    "- numpy is the main package for scientific computing with Python.\n",
    "\n",
    "- matplotlib is a library to plot graphs in Python.\n",
    "\n",
    "- dnn_utils provides some necessary functions for this notebook.\n",
    "\n",
    "- testCases provides some test cases to assess the correctness of your functions\n",
    "\n",
    "- np.random.seed(1) is used to keep all the random function calls consistent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from testCases_v3 import *;\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward;\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (5.0, 4.0); # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest';\n",
    "plt.rcParams['image.cmap'] = 'gray';\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.random.seed(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Overview of the Problem set ##\n",
    "Details of datasets:\n",
    "\n",
    "In this data there are 2062 sign language digits images.\n",
    "\n",
    "Image size: 64x64\n",
    "\n",
    "Color space: Grayscale\n",
    "\n",
    "File format: npy\n",
    "\n",
    "Number of classes: 10 (Digits: 0-9)\n",
    "\n",
    "Number of participant students: 218\n",
    "\n",
    "Number of samples per student: 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 63.5, 63.5, -0.5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_l = np.load('C:\\\\Users\\\\Admin\\\\Desktop\\\\internship_course\\\\SELF\\\\X.npy')\n",
    "Y_l = np.load('C:\\\\Users\\\\Admin\\\\Desktop\\\\internship_course\\\\SELF\\\\Y.npy')\n",
    "img_size = 64\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(x_l[260].reshape(img_size, img_size))\n",
    "plt.axis('off')\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(x_l[2000].reshape(img_size, img_size))\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compute : $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    " \"\"\"\n",
    " Implements the sigmoid activation in numpy\n",
    " Arguments:\n",
    " Z -- numpy array of any shape\n",
    " Returns:\n",
    " A -- output of sigmoid(z), same shape as Z\n",
    " cache -- returns Z as well, useful during backpropagation\n",
    " \"\"\"\n",
    " A = 1 / (1 + np.exp(-Z));\n",
    " cache = Z;\n",
    " return A, cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    " \"\"\"\n",
    " Implement the backward propagation for a single SIGMOID unit.\n",
    " Arguments:\n",
    " dA -- post-activation gradient, of any shape\n",
    " cache -- 'Z' where we store for computing backward propagation efficiently\n",
    " Returns:\n",
    " dZ -- Gradient of the cost with respect to Z\n",
    " \"\"\"\n",
    " Z = cache;\n",
    " s = 1 / (1 + np.exp(-Z));\n",
    " dZ = dA * s * (1 - s);\n",
    " assert (dZ.shape == Z.shape);\n",
    " return dZ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    " \"\"\"\n",
    " Implement the RELU function.\n",
    " Arguments:\n",
    " Z -- Output of the linear layer, of any shape\n",
    " Returns:\n",
    " A -- Post-activation parameter, of the same shape as Z\n",
    " cache -- a python dictionary containing \"A\" ; stored for computing the backward pass effic\n",
    " \"\"\"\n",
    " A = np.maximum(0,Z);\n",
    " assert(A.shape == Z.shape);\n",
    " cache = Z;\n",
    " return A, cache;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    " \"\"\"\n",
    " Implement the backward propagation for a single RELU unit.\n",
    " Arguments:\n",
    " dA -- post-activation gradient, of any shape\n",
    " cache -- 'Z' where we store for computing backward propagation efficiently\n",
    " Returns:\n",
    " dZ -- Gradient of the cost with respect to Z\n",
    " \"\"\"\n",
    " Z = cache;\n",
    " dZ = np.array(dA, copy = True); # just converting dz to a correct object.\n",
    " # When z <= 0, you should set dz to 0 as well.\n",
    " dZ[Z <= 0] = 0;\n",
    " assert (dZ.shape == Z.shape);\n",
    " return dZ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (410, 64, 64)\n",
      "Y shape:  (410, 1)\n"
     ]
    }
   ],
   "source": [
    "# Join a sequence of arrays along an row axis.\n",
    "X = np.concatenate((x_l[204:409], x_l[822:1027] ), axis=0) # from 0 to 204 is zero sign and from 205 to 410 is one sign \n",
    "z = np.zeros(205)\n",
    "o = np.ones(205)\n",
    "Y = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)\n",
    "print(\"X shape: \" , X.shape)\n",
    "print(\"Y shape: \" , Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then lets create x_train, y_train, x_test, y_test arrays\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "number_of_train = X_train.shape[0]\n",
    "number_of_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train flatten (348, 4096)\n",
      "X test flatten (62, 4096)\n"
     ]
    }
   ],
   "source": [
    "X_train_flatten = X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2])\n",
    "X_test_flatten = X_test .reshape(number_of_test,X_test.shape[1]*X_test.shape[2])\n",
    "print(\"X train flatten\",X_train_flatten.shape)\n",
    "print(\"X test flatten\",X_test_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train:  (4096, 348)\n",
      "x test:  (4096, 62)\n",
      "y train:  (1, 348)\n",
      "y test:  (1, 62)\n"
     ]
    }
   ],
   "source": [
    "x_train = X_train_flatten.T\n",
    "x_test = X_test_flatten.T\n",
    "y_train = Y_train.T\n",
    "y_test = Y_test.T\n",
    "print(\"x train: \",x_train.shape)\n",
    "print(\"x test: \",x_test.shape)\n",
    "print(\"y train: \",y_train.shape)\n",
    "print(\"y test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer Size and Weights and Bias Initialization** <BR>\n",
    "In x_train, there are 348 images x^(348)<BR>\n",
    "To obtain variety, we should initialize weights randomly; but as small number like 0.01. <br>\n",
    "We should initialize weights as small number since when we use tanh activation function with big value weights, its derivative is approximetly 0 and we cannot generate optimistic result. (ex.: tanh(5) ~= 0.999 and Its derivative is ~ 0).\n",
    "However we can initialize bias as zero.\n",
    "I prefer to use 3 nodes hidden layer when implementing 2 layer ANN. Because of this reason:\n",
    " * Shape of weight1 :  (3, 4096)\n",
    " * Shape of weight2 :  (1, 3)\n",
    " * Shape of bias1 :  (3, 1)\n",
    " * Shape of bias2 :  (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weight1 :  (3, 4096)\n",
      "Shape of weight2 :  (1, 3)\n",
      "Shape of bias1 :  (3, 1)\n",
      "Shape of bias2 :  (1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'weight1': array([[ 0.16243454, -0.06117564, -0.05281718, ..., -0.14788339,\n",
       "         -0.04929046,  0.07949473],\n",
       "        [-0.10922135,  0.20865146,  0.1316653 , ..., -0.06137821,\n",
       "          0.13543076,  0.05320339],\n",
       "        [-0.06501626,  0.22848077, -0.0540921 , ..., -0.05272142,\n",
       "         -0.03803397,  0.09494124]]), 'bias1': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]), 'weight2': array([[ 0.10092311,  0.0229889 , -0.06640991]]), 'bias2': array([[0.]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parameter_initialize(x_train,y_train):\n",
    "    parameters = {\"weight1\": np.random.randn(3,x_train.shape[0])*0.1,\n",
    "                  \"bias1\": np.zeros((3,1)),\n",
    "                  \"weight2\": np.random.randn(y_train.shape[0],3)*0.1,\n",
    "                  \"bias2\" : np.zeros((y_train.shape[0],1))}\n",
    "    \n",
    "    print(\"Shape of weight1 : \",parameters[\"weight1\"].shape)\n",
    "    print(\"Shape of weight2 : \",parameters[\"weight2\"].shape)\n",
    "    print(\"Shape of bias1 : \",parameters[\"bias1\"].shape)\n",
    "    print(\"Shape of bias2 : \",parameters[\"bias2\"].shape)\n",
    "\n",
    "    return parameters\n",
    "parameter_initialize(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Propagation**<br>\n",
    "In this part we use two different activation functions which are sigmoid and tanh.\n",
    "Firstly, we should initialize sigmoid function and for tanh function we use numpy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    A = 1/(1+np.exp(-z))\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x_train,parameters):\n",
    "    Z1 = np.dot(parameters[\"weight1\"],x_train) + parameters[\"bias1\"]\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    results = {\"Z1\": Z1, \"A1\":A1,\"Z2\":Z2,\"A2\":A2}\n",
    "    return A2, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(A2,Y):\n",
    "    logaritmic_probability = np.multiply(np.log(A2),Y)\n",
    "    cost = -np.sum(logaritmic_probability)/Y.shape[1]\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward Propagation** <BR>\n",
    " In this part, we take derivative of weights, for weight1 and weight2, bais, for bais1 and bais2, results, for Z1 and Z2 according to cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters,results,X,Y):\n",
    "    dZ2 = results[\"A2\"]-Y\n",
    "    dW2 = np.dot(dZ2,results[\"A1\"].T)/X.shape[1]\n",
    "    db2 = np.sum(dZ2, axis = 1, keepdims = True)/X.shape[1]\n",
    "    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1-np.power(results[\"A1\"],2))\n",
    "    dW1 = np.dot(dZ1,X.T)/X.shape[1]\n",
    "    db1 = np.sum(dZ1, axis=1,keepdims = True)/X.shape[1]\n",
    "    gradients = {\"dweight1\": dW1,\n",
    "                \"dweight2\": dW2,\n",
    "                \"dbias1\": db1,\n",
    "                \"dbias2\":db2}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prameters(parameters,grand,learning_rate = 0.01):\n",
    "    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grand[\"dweight1\"],\n",
    "                  \"bias1\": parameters[\"bias1\"]-learning_rate*grand[\"dbias1\"],\n",
    "                  \"weight2\": parameters[\"weight2\"]-learning_rate*grand[\"dweight2\"],\n",
    "                  \"bias2\" : parameters[\"bias2\"]-learning_rate*grand[\"dbias2\"]\n",
    "                 }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(parameters, x_test):\n",
    "    A2, results = forward_propagation(x_test,parameters)\n",
    "    prediction = np.zeros((1,x_test.shape[1]))\n",
    "    \n",
    "    for i in range(A2.shape[1]):\n",
    "        if A2[0,i] <= 0.5:\n",
    "            prediction[0,i] = 0\n",
    "        else:\n",
    "            prediction[0,i] = 1\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Layer ANN Model Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weight1 :  (3, 4096)\n",
      "Shape of weight2 :  (1, 3)\n",
      "Shape of bias1 :  (3, 1)\n",
      "Shape of bias2 :  (1, 1)\n",
      "Cost after iteration 0 0.342632\n",
      "Cost after iteration 100 0.316843\n",
      "Cost after iteration 200 0.278211\n",
      "Cost after iteration 300 0.222048\n",
      "Cost after iteration 400 0.171436\n",
      "Cost after iteration 500 0.135530\n",
      "Cost after iteration 600 0.111432\n",
      "Cost after iteration 700 0.094889\n",
      "Cost after iteration 800 0.082982\n",
      "Cost after iteration 900 0.073971\n",
      "Cost after iteration 1000 0.066836\n",
      "Cost after iteration 1100 0.060947\n",
      "Cost after iteration 1200 0.055879\n",
      "Cost after iteration 1300 0.051316\n",
      "Cost after iteration 1400 0.047013\n",
      "Cost after iteration 1500 0.042827\n",
      "Cost after iteration 1600 0.038785\n",
      "Cost after iteration 1700 0.034991\n",
      "Cost after iteration 1800 0.031447\n",
      "Cost after iteration 1900 0.028107\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 99.42528735632185 %\n",
      "test accuracy: 96.7741935483871 %\n"
     ]
    }
   ],
   "source": [
    "def two_layer_ANN_model(x_train, y_train, x_test, y_test, number_of_iteration):\n",
    "    cost_list = []\n",
    "    index = []\n",
    "    parameters = parameter_initialize(x_train,y_train)\n",
    "    for i in range(number_of_iteration):\n",
    "        A2, results = forward_propagation(x_train,parameters)\n",
    "        cost_result = cost(A2,y_train)\n",
    "        gradients = backward_propagation(parameters,results,x_train,y_train)\n",
    "        parameters = update_prameters(parameters, gradients)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            cost_list.append(cost_result)\n",
    "            index.append(i)\n",
    "            print(\"Cost after iteration %i %f\" %(i,cost_result))\n",
    "    plt.plot(index,cost_list)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Ä±teration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    y_prediction_test = prediction(parameters,x_test)\n",
    "    y_prediction_train = prediction(parameters,x_train)\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
    "    return parameters\n",
    "parameters = two_layer_ANN_model(x_train,y_train,x_test,y_test,2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    " \"\"\"\n",
    " Implements the sigmoid activation in numpy\n",
    " Arguments:\n",
    " Z -- numpy array of any shape\n",
    " Returns:\n",
    " A -- output of sigmoid(z), same shape as Z\n",
    " cache -- returns Z as well, useful during backpropagation\n",
    " \"\"\"\n",
    " A = 1 / (1 + np.exp(-Z));\n",
    " cache = Z;\n",
    " return A, cache;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "       \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "       \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 - Forward propagation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "   \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, activation_cache = sigmoid(Z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, activation_cache = relu(Z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "     \n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "     \n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L layered**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "       \n",
    "        W = parameters['W' + str(l)]\n",
    "        b = parameters['b' + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "   \n",
    "    W = parameters['W' + str(L)]\n",
    "    b = parameters['b' + str(L)]\n",
    "    AL, cache = linear_activation_forward(A, W, b, activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**compute cost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (â‰ˆ 1 lines of code)\n",
    "    cost = (-1/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_test_case():\n",
    "    Y = np.asarray([[1, 1, 1]]);\n",
    "    aL = np.array([[.8,.9,0.4]]);\n",
    "    return Y, aL;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    " \"\"\"\n",
    " Implement the backward propagation for a single RELU unit.\n",
    " Arguments:\n",
    " dA -- post-activation gradient, of any shape\n",
    " cache -- 'Z' where we store for computing backward propagation efficiently\n",
    " Returns:\n",
    " dZ -- Gradient of the cost with respect to Z\n",
    " \"\"\"\n",
    " Z = cache;\n",
    " dZ = np.array(dA, copy = True); # just converting dz to a correct object.\n",
    " # When z <= 0, you should set dz to 0 as well.\n",
    " dZ[Z <= 0] = 0;\n",
    " assert (dZ.shape == Z.shape);\n",
    " return dZ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    print(\"L = \"+str(L))\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    print(\"dA\"+ str(L-1)+\" = \"+str(grads[\"dA\" + str(L-1)]))\n",
    "    print(\"dW\"+ str(L)+\" = \"+str(grads[\"dW\" + str(L)]))\n",
    "    print(\"db\"+ str(L)+\" = \"+str(grads[\"db\" + str(L)]))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**update parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train accuracy: 98.4 % <BR>\n",
    "test accuracy: 95.7 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on implementing all the functions required for building a deep neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
